{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Prac1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPD/tzIoXuQqI+x1YsPMhYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshilgandhi3199/LinearRegression/blob/master/ML_Prac1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po58IHFkfQ7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8d2c35c-92c8-47ea-84d5-6872c4a94bd0"
      },
      "source": [
        "#Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataset=pd.read_csv(\"/content/Bengaluru_House_Data.csv\")\n",
        "dataset.head()\n",
        "\n",
        "#check if any missing or null values in the dataset\n",
        "dataset.isnull().sum() \n",
        "dataset.shape #(13274,9)\n",
        "\n",
        "#column \"soceity\" and has more number of null values and it does not account for any coorelation between variables thus removing it \n",
        "#also \"availibity\" does not account for much change in price thus removing it\n",
        "#removing \"location\" becasue it has large number of categories and encoding it will be of no use as it will on add to complexity.\n",
        "df=dataset.drop(columns=['society', 'availability', 'location'])\n",
        "df.shape #(13320,6)\n",
        "\n",
        "#cleaning \"size\" col i.e 2BHK -> 2\n",
        "import re\n",
        "df['size'].fillna(\"nan\", inplace=True)\n",
        "for i in range(len(df)):\n",
        "  if df.iloc[i,1]!=\"nan\":\n",
        "    temp=re.findall(r'\\d+', df._get_value(i, 'size'))\n",
        "    temp=list(map(int, temp))\n",
        "    temp=temp[0]\n",
        "    df._set_value(i, 'size', temp)\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "#dropping rows which have Nan\n",
        "df=df.dropna()\n",
        "df.head()\n",
        "df.shape #(12669,6)\n",
        "\n",
        "X=df.iloc[:, 0:5].values\n",
        "y=df.iloc[:, -1].values\n",
        "X\n",
        "\n",
        "#encoding categorical data i.e string to 0,1,.. and so on\n",
        "#Label encoding 0th col -> onehot encoding 0th col using ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "labelEncoder=LabelEncoder()\n",
        "X[:,0]=labelEncoder.fit_transform(X[:,0])\n",
        "transformer=ColumnTransformer(transformers=[(\"OneHot\", OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X=transformer.fit_transform(X.tolist())\n",
        "#avoding dummy variable trap\n",
        "X=X[:,1:]\n",
        "\n",
        "#splitting the dataset into training and train set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "#normaliation - appyling min-max normalization\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "norm=MinMaxScaler().fit(X_train)\n",
        "X_train_norm=norm.transform(X_train)\n",
        "X_test_norm=norm.transform(X_test)\n",
        "\n",
        "#fitting simple linear regression to training set\n",
        "from sklearn.linear_model import LinearRegression\n",
        "regressor=LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "#predicting the test set\n",
        "y_pred=regressor.predict(X_test)\n",
        "\n",
        "#score\n",
        "print(\"Regressor score: %.2f\" % regressor.score(X_test, y_test))\n",
        "\n",
        "#calculating mean squared error, coefficients and r2score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "print(\"Mean_squared_error:  %.2f\" % mean_squared_error(y_test,y_pred))\n",
        "print(\"Coeefficient of determination:  %.2f\" % r2_score(y_test, y_pred))\n",
        "print(\"Coefficients: \\n\", regressor.coef_)\n",
        "\n",
        "#finding the correlation between features using heatmap\n",
        "corr_matrix=df.corr()\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "#we can viualize from the heat map that \"total_sqft\" and \"price\" columns are highly correlated and thus \"total_sqft\" will have\n",
        "#larger influence on the price of the property\n",
        "\n",
        "#determining the pearson's correlation between \"total_sqft\" and \"price\" \n",
        "from scipy.stats import pearsonr\n",
        "corr,_=pearsonr(df['total_sqft'], df['price'])    #corr (total_sqft<->price = 0.558)\n",
        "print(\"Pearson correlation: %.3f\" % corr)  #heat map shows the similar results\n",
        "\n",
        "x1=X_test.astype('float64')[:,4]\n",
        "y1=y_test.astype('float64')\n",
        "\n",
        "#visualizing test set results for variable \"bath\" and \"price\" because they comparatively more \n",
        "#correlated than others\n",
        "#best fit line using polyfit()\n",
        "m, b = np.polyfit(x1, y1, 1)\n",
        "print(\"Slope: \" + str(m) + \" Intercept: \" + str(b)) \n",
        "plt.scatter(X_test[1:15, 4], y_test[1:15], color='red')                       #scatter it along x and y axis\n",
        "plt.plot(X_test[1:15, 4], m*X_test[1:15, 4] + b, color=\"blue\")\n",
        "#plt.plot(X_test[1:15, 5], y_pred[1:15], color='blue', linewidth=3)\n",
        "plt.xlabel(\"Total_sqft\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.show()\n",
        "\n",
        "#variables that are independent\n",
        "#correlation between variables\n",
        "\"\"\"\n",
        "           | total_sqft \tbath \tbalcony \tprice\n",
        "total_sqft |\t1.000000 \t0.394496 \t0.145870 \t0.558449\n",
        "bath       |  0.394496 \t1.000000 \t0.203675 \t0.459314\n",
        "balcony    | \t0.145870 \t0.203675 \t1.000000 \t0.120400\n",
        "price      |  0.558449 \t0.459314 \t0.120400 \t1.000000\n",
        "\"\"\"\n",
        "\n",
        "#we can say from above correlation matrix that variables \"size\" and \"area_type\" are independent\n",
        "#and does not consitute for the any correlation. \n",
        "\n",
        "\n",
        "#important sources\n",
        "\"\"\"\n",
        "sklearn website for classes and methods\n",
        "geeks for geeks - data cleaning and certain other operations\n",
        "sns website - heatmap\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Regressor score: 0.36\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}